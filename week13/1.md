## Monitoring
[What is Monitoring/Logging]

Logging is the practice of managing all of the log data produced by your applications and infrastructure.

Put simply, monitoring is the art and science of ensuring that an application both remains available and responds to user requests within an acceptable amount of time. More broadly speaking, monitoring can also involve goals such as optimizing code or reducing infrastructure costs, but we’ll stick to the simpler definition for now.

{Logging and monitoring are both valuable components to maintaining optimal application performance. Using a combination of logging tools and real-time monitoring systems helps improve observability and reduces the time spent sifting through log files to determine the root cause of performance problems.}

[Why do we need Monitoring/Logging]

- Your system can be your website, errors, user events, access to your server, or system errors. If you can name it, there's a log for it! You can use logs to keep track of many things. Similarly, logs can help you prevent vulnerabilities, raise alerts for bottlenecks, improve services, and so on.

- Logging is the process of providing information about an application as it performs different tasks or events. Logging offers benefits such as: Issue Diagnosis: Let's say a bug was reported by a user and you want to replicate that scenario in your development environment.10 באוג׳ 2020


[Tools for monitoring ](Prometheus Stack, Elastic Stack, Datadog, Zabbix, Splunk)

Datadog is a monitoring and analytics tool for information technology (IT) and DevOps teams that can be used to determine performance metrics as well as event monitoring for infrastructure and cloud services. The software can monitor services such as servers, databases and tools.

Zabbix is an open source monitoring software tool for diverse IT components, including networks, servers, virtual machines (VMs) and cloud services. Zabbix provides monitoring metrics, such as network utilization, CPU load and disk space consumption.

Splunk is used for monitoring and searching through big data. It indexes and correlates information in a container that makes it searchable, and makes it possible to generate alerts, reports and visualizations.


## Prometheus Stack
kube-prometheus-stack collects Kubernetes manifests, Grafana dashboards, and Prometheus rules combined with documentation and scripts to provide easy to operate end-to-end Kubernetes cluster monitoring with Prometheus using the Prometheus Operator.

[Stack Architecture]
Prometheus scrapes metrics from a variety of application endpoints and stores them in its internal database. Grafana then reads these metrics from Prometheus and displays visualizations and dashboards based on these metrics on its UI. Prometheus alerts can also be configured to send notifications to external systems, such that when an event occurs, Prometheus sends the alert data to the Alertmanager which in turn dynamically routes the alerts to different receivers such as email, Slack, or PagerDuty.

[Prometheus]
Prometheus, a Cloud Native Computing Foundation project, is a systems and service monitoring system. It collects metrics from configured targets at given intervals, evaluates rule expressions, displays the results, and can trigger alerts when specified conditions are observed.

The features that distinguish Prometheus from other metrics and monitoring systems are:

A multi-dimensional data model (time series defined by metric name and set of key/value dimensions)
PromQL, a powerful and flexible query language to leverage this dimensionality
No dependency on distributed storage; single server nodes are autonomous
An HTTP pull model for time series collection
Pushing time series is supported via an intermediary gateway for batch jobs
Targets are discovered via service discovery or static configuration
Multiple modes of graphing and dashboarding support
Support for hierarchical and horizontal federation

[Targets]
Prometheus is a monitoring platform that collects metrics from monitored targets by scraping metrics HTTP endpoints on these targets.

Prometheus has a scraping configuration that allows you to add target you want to scrape. This is the documentation (a good starting point). Thanks for the answer. I actually fixed it mapping these configs by editing the configmap.


[Metrics]
While data is merely just a number, a metric is a quantitative measurement of data, in relation to what you are actually measuring. Your data point maybe just a number, but your metric is the number of minutes or hours.

Metrics are measures of quantitative assessment commonly used for comparing, and tracking performance or production. Metrics can be used in a variety of scenarios. Metrics are heavily relied on in the financial analysis of companies by both internal managers and external stakeholders.

What is Prometheus? Prometheus is a metrics collection and alerting tool developed and released to open source by SoundCloud. Prometheus is similar in design to Google's Borgmon monitoring system, and a relatively modest system can handle collecting hundreds of thousands of metrics every second.

You can also verify that Prometheus is serving metrics about itself by navigating to its own metrics endpoint: http://localhost:9090/metrics.

Prometheus uses a very simple metric model with four metric types that are only supported in the client libraries.
The researchers have determined that only four key metrics differentiate between low, medium and high performers: lead time, deployment frequency, mean time to restore (MTTR) and change fail percentage.

[Exporters]
Exporters are essential pieces within a Prometheus monitoring environment. Each program acting as a Prometheus client holds an exporter at its core. An exporter is comprised of software features that produce metrics data, and an HTTP server that exposes the generated metrics available via a given endpoint.

A Prometheus Exporter is a piece of software that

Can fetch statistics from another, non-Prometheus system
Can turn those statistics into Prometheus metrics, using a client library
Starts a web server that exposes a /metrics URL, and have that URL display the system metrics
To a non-programmer, a Prometheus Exporter seems to allow you to export statistics from a non-Prometheus system and import them to Prometheus. To a programmer, a Prometheus Exporter is really more like a content scraper that converts metrics from one format to another.

[AlertManager]
The Alertmanager handles alerts sent by client applications such as the Prometheus server. It takes care of deduplicating, grouping, and routing them to the correct receiver integration such as email, PagerDuty, or OpsGenie. It also takes care of silencing and inhibition of alerts.

To Configure AlertManager we need to follow these steps,
Step 1 - Configure Alert Rule. ...
Step 2 - AlertManager Config File. ...
Step 3 - Update prometheus.yml file to configure Prometheus server to talk with AlertManager Service. ...
Step 4 - Update docker-compose.yml File. ...
Step 5 - Run the container.

[Alerts]
Alerting with Prometheus is separated into two parts. Alerting rules in Prometheus servers send alerts to an Alertmanager. The Alertmanager then manages those alerts, including silencing, inhibition, aggregation and sending out notifications via methods such as email, on-call notification systems, and chat platforms.


[Rules]
Prometheus supports two types of rules which may be configured and then evaluated at regular intervals: recording rules and alerting rules. To include rules in Prometheus, create a file containing the necessary rule statements and have Prometheus load the file via the rule_files field in the Prometheus configuration.


[Grafana]
Grafana is an open source interactive data-visualization platform, developed by Grafana Labs, which allows users to see their data via charts and graphs that are unified into one dashboard (or multiple dashboards!) for easier interpretation and understanding.
Grafana is a multi-platform open source analytics and interactive visualization web application. It provides charts, graphs, and alerts for the web when connected to supported data sources.


[Dashboards]
A Grafana dashboard is a powerful open source analytical and visualization tool that consists of multiple individual panels arranged in a grid. The panels interact with configured data sources, including (but not limited to) AWS CloudWatch, Microsoft SQL server, Prometheus, MySQL, InfluxDB, and many others.


## Elastic Stack
The Elastic Stack is a group of open source products from Elastic designed to help users take data from any type of source and in any format, and search, analyze and visualize that data in real time.

[Stack Architecture]
The Elastic Stack is designed to help users import data of any format and source and to search, analyze, and visualize imported data in real time. The following figure shows the components available in the Elastic Stack: Beats are agents that ship the data from different systems.

The Elasticsearch architecture is designed to support the retrieval of documents, which are stored as JSON objects. Elasticsearch supports nested structures, which helps handle complex data and queries. To track information, Elasticsearch uses keys prepended with an underscore, which represents metadata.


[Elasticsearch]
Elasticsearch allows you to store, search, and analyze huge volumes of data quickly and in near real-time and give back answers in milliseconds. It's able to achieve fast search responses because instead of searching the text directly, it searches an index.

Elasticsearch is a distributed search and analytics engine built on Apache Lucene. Since its release in 2010, Elasticsearch has quickly become the most popular search engine and is commonly used for log analytics, full-text search, security intelligence, business analytics, and operational intelligence use cases.

[Indices]
Elasticsearch takes in unstructured data from different locations, stores and indexes it according to user-specified mapping (which can also be derived automatically from data), and makes it searchable. Its distributed architecture makes it possible to search and analyze huge volumes of data in near real time.

In Elasticsearch, an index (plural: indices) contains a schema and can have one or more shards and replicas. An Elasticsearch index is divided into shards and each shard is an instance of a Lucene index. Indices are used to store the documents in dedicated data structures corresponding to the data type of fields.

Indexes are stored on disk as configured in elasticsearch. yml with the configuration option path. data ; localhost on port 9200 is the default connection port for the HTTP REST interface, the path of the url generally defines an action to be taken (like searching for documents);

[Documents]
Elasticsearch is a distributed document store. Instead of storing information as rows of columnar data, Elasticsearch stores complex data structures that have been serialized as JSON documents.

Elasticsearch is a document oriented database. The entire object graph you want to search needs to be indexed, so before indexing your documents, they must be denormalized.


[API]
This type of Elasticsearch API allows users to manage indices, mappings, and templates. For example, you can use this API to create or delete a new index, check if a specific index exists or not, and define new mapping for an index.

There are two ways to connect to your Elasticsearch cluster: Through the RESTful API or through the Java transport client. Both ways use an endpoint URL that includes a port, such as https://ec47fc4d2c53414e1307e85726d4b9bb.us-east-1.aws.found.io:9243 .


[Kibana] ELK elastiksearch logstash kibana
Elasticsearch is a search and analytics engine. Logstash is a server‑side data processing pipeline that ingests data from multiple sources simultaneously, transforms it, and then sends it to a "stash" like Elasticsearch. Kibana lets users visualize data with charts and graphs in Elasticsearch.

Kibana is a data visualization and exploration tool used for log and time-series analytics, application monitoring, and operational intelligence use cases. It offers powerful and easy-to-use features such as histograms, line graphs, pie charts, heat maps, and built-in geospatial support.

[Data Visualization]
What is Kibana used for? Kibana's tight integration with Elasticsearch and the larger Elastic Stack make it ideal for supporting the following: Searching, viewing, and visualizing data indexed in Elasticsearch and analyzing the data through the creation of bar charts, pie charts, tables, histograms, and maps.

Data visualization is the practice of translating information into a visual context, such as a map or graph, to make data easier for the human brain to understand and pull insights from. The main goal of data visualization is to make it easier to identify patterns, trends and outliers in large data sets.


[Beats]

Beats is a free and open platform for single-purpose data shippers. They send data from hundreds or thousands of machines and systems to Logstash or Elasticsearch.

There are currently six official Beats from Elastic: Filebeat, Metricbeat, Packetbeat, Heartbeat, Winlogbeat, and Auditbeat. All of these beats are open source and Apache-licensed.

Beats are built on top of a Go framework called libbeat — a library for data forwarding which means that new beats are being developed and contributed by the community all the time. As Elastic makes changes to each of these shippers, we will keep up to date to highlight certain features available with each one.


[Transporting data]

Kibana provides the capabilities to export saved objects created by the user using the Management menu. You can export saved dashboards, search results, visualisations and more inside the Saved Objects submenu. You can filter by the type of export using the Types dropdown menu on the right of the search box.

From the raddec index choose the fields of data you want to export by feeding the Selected Fields list. Add an Available field by clicking the Add button when the mouse is over it. Once the Selected Fields list is complete, Save it from the top menu bar. Choose a Name that will be the name of the CSV file generated.


There are several ways in which you can export data from Elasticsearch, depending upon the type of file you want to export.
...
Elasticsearch Export: Using Logstash-Input-Elasticsearch Plugin. ...
Elasticsearch Export: Using Elasticsearch Dump. ...
Elasticsearch Export: Using Python Pandas.


Logstash is a light-weight, open-source, server-side data processing pipeline that allows you to collect data from a variety of sources, transform it on the fly, and send it to your desired destination. It is most often used as a data pipeline for Elasticsearch, an open-source analytics and search engine.


